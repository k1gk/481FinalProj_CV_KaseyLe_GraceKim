{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f1bdb4-1b3f-4add-8a26-008190c8cb45",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20454dfa-74b9-47e2-848c-04826cd8c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import shutil\n",
    "import ultralytics\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acbf491-1f63-4a0d-879a-d66672b8782a",
   "metadata": {},
   "source": [
    "Creating Directories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e3c3a-ad05-4445-939d-e1dffce3e2ae",
   "metadata": {},
   "source": [
    "Dataset\n",
    "\n",
    "1. WikiArt Art Movements/Styles(\"https://www.kaggle.com/datasets/sivarazadi/wikiart-art-movementsstyles?resource=download-directory&select=Romanticism\")\n",
    "   - Baroque: 1000 files\n",
    "   - Expressionism: 940 original files\n",
    "   - Realism: 1000 files\n",
    "   - Renaissance: 1000 files\n",
    "   - Romanticism: 1000 files\n",
    "2. Manual Data Collection: we created a new class called 'Cubism' and provided more files for Expressionism as it \n",
    "   - Cubism: 1000 files\n",
    "   - Expressionism: 1000 files (additional 60 files)\n",
    "3. Prediction Images\n",
    "   - We used the 3 unused images from dataset per class for prediction section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb73765-622b-43ac-8300-a115f6119b64",
   "metadata": {},
   "source": [
    "Folder Organization\n",
    "- Train: 80% of dataset\n",
    "- Val: 20% of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d4f63be-6aa2-4055-89c1-971ac156fcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [\n",
    "    '../Downloads/RefinedArtStyles/train',\n",
    "    '../Downloads/RefinedArtStyles/val',\n",
    "    \n",
    "    '../Downloads/RefinedArtStyles/train/Baroque',\n",
    "    '../Downloads/RefinedArtStyles/train/Cubism',\n",
    "    '../Downloads/RefinedArtStyles/train/Expressionism',\n",
    "    '../Downloads/RefinedArtStyles/train/Realism',\n",
    "    '../Downloads/RefinedArtStyles/train/Renaissance',\n",
    "    '../Downloads/RefinedArtStyles/train/Romanticism',\n",
    "\n",
    "    '../Downloads/RefinedArtStyles/val/Baroque',\n",
    "    '../Downloads/RefinedArtStyles/val/Cubism',\n",
    "    '../Downloads/RefinedArtStyles/val/Expressionism',\n",
    "    '../Downloads/RefinedArtStyles/val/Realism',\n",
    "    '../Downloads/RefinedArtStyles/val/Renaissance',\n",
    "    '../Downloads/RefinedArtStyles/val/Romanticism'    \n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41b07065-485c-4c25-8cef-48057bc08a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "baroque_source = '../Downloads/RefinedArtStyles/Baroque'\n",
    "baroque_train = '../Downloads/RefinedArtStyles/train/Baroque'\n",
    "baroque_val = '../Downloads/RefinedArtStyles/val/Baroque'\n",
    "\n",
    "expression_source = '../Downloads/RefinedArtStyles/Expressionism'\n",
    "expression_train = '../Downloads/RefinedArtStyles/train/Expressionism'\n",
    "expression_val = '../Downloads/RefinedArtStyles/val/Expressionism'\n",
    "\n",
    "realism_source = '../Downloads/RefinedArtStyles/Realism'\n",
    "realism_train = '../Downloads/RefinedArtStyles/train/Realism'\n",
    "realism_val = '../Downloads/RefinedArtStyles/val/Realism'\n",
    "\n",
    "renaissance_source = '../Downloads/RefinedArtStyles/Renaissance'\n",
    "renaissance_train = '../Downloads/RefinedArtStyles/train/Renaissance'\n",
    "renaissance_val = '../Downloads/RefinedArtStyles/val/Renaissance'\n",
    "\n",
    "romanticism_source = '../Downloads/RefinedArtStyles/Romanticism'\n",
    "romanticism_train = '../Downloads/RefinedArtStyles/train/Romanticism'\n",
    "romanticism_val = '../Downloads/RefinedArtStyles/val/Romanticism'\n",
    "\n",
    "cubism_source = '../Downloads/RefinedArtStyles/Cubism'\n",
    "cubism_train = '../Downloads/RefinedArtStyles/train/Cubism'\n",
    "cubism_val = '../Downloads/RefinedArtStyles/val/Cubism'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9defe6-ef9b-46f9-9ab6-5ba7b99d570e",
   "metadata": {},
   "source": [
    "Dataset Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76bba056-b08d-4b7b-8b8e-311466abb24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def organize_file(source_dir, train_dir, val_dir):\n",
    "    files = []\n",
    "    for f in os.listdir(source_dir):\n",
    "        files.append(f)\n",
    "\n",
    "    if len(files) == 0:\n",
    "        print(\"No files found in the source directory:\", source_dir)\n",
    "        return\n",
    "    \n",
    "    train_files, val_files = train_test_split(files, test_size=0.2, random_state=42)\n",
    "    \n",
    "    for file in train_files:\n",
    "        file_path = os.path.join(source_dir, file)\n",
    "        image = Image.open(file_path).convert(\"RGB\")\n",
    "        image.save(os.path.join(train_dir, file), \"JPEG\") \n",
    "        \n",
    "    for file in val_files:\n",
    "        file_path = os.path.join(source_dir, file)\n",
    "        image = Image.open(file_path).convert(\"RGB\")\n",
    "        image.save(os.path.join(val_dir, file), \"JPEG\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d590888b-4cf0-4577-a667-da6b81d78aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "organize_file(baroque_source, baroque_train, baroque_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4fc41ea-5f65-44ef-8aa2-52bb6b92840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "organize_file(expression_source, expression_train, expression_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2ff87d5-3b24-4590-bfa0-e78edc32074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "organize_file(realism_source, realism_train, realism_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50ffd6e4-e07a-4939-841a-38214020f927",
   "metadata": {},
   "outputs": [],
   "source": [
    "organize_file(renaissance_source, renaissance_train, renaissance_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f995f911-a3f8-4bf4-b84c-b5df3288339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "organize_file(romanticism_source, romanticism_train, romanticism_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a676f490-88cb-4394-8bc2-bdc45f8225fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "organize_file(cubism_source, cubism_train, cubism_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2373f821-1048-4965-9ae3-1407605ba842",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "053c7251-5293-4af8-a8b2-07c633c783df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.47  Python-3.11.7 torch-2.2.2+cpu CPU (11th Gen Intel Core(TM) i9-11900K 3.50GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=RefinedArtStyles, epochs=3, time=None, patience=100, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train4, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\classify\\train4\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... found 4800 images in 6 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... found 1200 images in 6 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=1000 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    337926  ultralytics.nn.modules.head.Classify         [256, 6]                      \n",
      "YOLOv8n-cls summary: 99 layers, 1445974 parameters, 1445974 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\classify\\train4', view at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... 4800 images, 0 corrupt: 100%|██████████| 4800/4800 [\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... 1200 images, 0 corrupt: 100%|██████████| 1200/1200 [00:0\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\classify\\train4\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/3         0G      1.364         16        224: 100%|██████████| 300/300 [03:26<00:00,  1.45it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 38/38 [00:35<00:00,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.668      0.997\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        2/3         0G     0.8824         16        224: 100%|██████████| 300/300 [03:22<00:00,  1.48it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 38/38 [00:35<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.753      0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/3         0G     0.7443         16        224: 100%|██████████| 300/300 [03:16<00:00,  1.53it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 38/38 [00:34<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.802      0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3 epochs completed in 0.198 hours.\n",
      "Optimizer stripped from runs\\classify\\train4\\weights\\last.pt, 3.0MB\n",
      "Optimizer stripped from runs\\classify\\train4\\weights\\best.pt, 3.0MB\n",
      "\n",
      "Validating runs\\classify\\train4\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.1.47  Python-3.11.7 torch-2.2.2+cpu CPU (11th Gen Intel Core(TM) i9-11900K 3.50GHz)\n",
      "YOLOv8n-cls summary (fused): 73 layers, 1442566 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... found 4800 images in 6 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... found 1200 images in 6 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               classes   top1_acc   top5_acc: 100%|██████████| 38/38 [00:33<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.804      0.999\n",
      "Speed: 0.0ms preprocess, 3.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\classify\\train4\u001b[0m\n",
      "Results saved to \u001b[1mruns\\classify\\train4\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
       "\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x0000022C67E785D0>\n",
       "curves: []\n",
       "curves_results: []\n",
       "fitness: 0.901666671037674\n",
       "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
       "results_dict: {'metrics/accuracy_top1': 0.8041666746139526, 'metrics/accuracy_top5': 0.9991666674613953, 'fitness': 0.901666671037674}\n",
       "save_dir: WindowsPath('runs/classify/train4')\n",
       "speed: {'preprocess': 0.000831286112467448, 'inference': 3.923502763112386, 'loss': 0.0, 'postprocess': 0.0}\n",
       "task: 'classify'\n",
       "top1: 0.8041666746139526\n",
       "top5: 0.9991666674613953"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = YOLO('yolov8n-cls.pt')\n",
    "model1.train(data='RefinedArtStyles', epochs=3, batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d365722-0731-4169-98bb-95c6278e4e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.47  Python-3.11.7 torch-2.2.2+cpu CPU (11th Gen Intel Core(TM) i9-11900K 3.50GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=RefinedArtStyles, epochs=5, time=None, patience=100, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\classify\\train5\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... found 4800 images in 6 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... found 1200 images in 6 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=1000 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    337926  ultralytics.nn.modules.head.Classify         [256, 6]                      \n",
      "YOLOv8n-cls summary: 99 layers, 1445974 parameters, 1445974 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\classify\\train5', view at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... 4800 images, 0 corrupt: 100%|██████████| 4800/4800 [\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... 1200 images, 0 corrupt: 100%|██████████| 1200/1200 [00:0\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\classify\\train5\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.364         16        224: 100%|██████████| 300/300 [03:23<00:00,  1.47it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 38/38 [00:35<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.668      0.997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        2/5         0G     0.8802         16        224: 100%|██████████| 300/300 [03:19<00:00,  1.50it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 38/38 [00:33<00:00,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.746      0.998\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        3/5         0G     0.7469         16        224: 100%|██████████| 300/300 [03:52<00:00,  1.29it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 38/38 [00:36<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.8      0.999\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        4/5         0G     0.6402         16        224: 100%|██████████| 300/300 [03:15<00:00,  1.53it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 38/38 [00:35<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.827          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        5/5         0G     0.5607         16        224: 100%|██████████| 300/300 [03:13<00:00,  1.55it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 38/38 [00:35<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.836          1\n",
      "\n",
      "5 epochs completed in 0.334 hours.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from runs\\classify\\train5\\weights\\last.pt, 3.0MB\n",
      "Optimizer stripped from runs\\classify\\train5\\weights\\best.pt, 3.0MB\n",
      "\n",
      "Validating runs\\classify\\train5\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.1.47  Python-3.11.7 torch-2.2.2+cpu CPU (11th Gen Intel Core(TM) i9-11900K 3.50GHz)\n",
      "YOLOv8n-cls summary (fused): 73 layers, 1442566 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... found 4800 images in 6 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... found 1200 images in 6 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               classes   top1_acc   top5_acc: 100%|██████████| 38/38 [00:33<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.836          1\n",
      "Speed: 0.0ms preprocess, 3.8ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\classify\\train5\u001b[0m\n",
      "Results saved to \u001b[1mruns\\classify\\train5\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
       "\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x0000022C6A34C1D0>\n",
       "curves: []\n",
       "curves_results: []\n",
       "fitness: 0.9179166555404663\n",
       "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
       "results_dict: {'metrics/accuracy_top1': 0.8358333110809326, 'metrics/accuracy_top5': 1.0, 'fitness': 0.9179166555404663}\n",
       "save_dir: WindowsPath('runs/classify/train5')\n",
       "speed: {'preprocess': 0.0024878978729248047, 'inference': 3.761421243349711, 'loss': 0.0, 'postprocess': 0.0}\n",
       "task: 'classify'\n",
       "top1: 0.8358333110809326\n",
       "top5: 1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = YOLO('yolov8n-cls.pt')\n",
    "model2.train(data='RefinedArtStyles', epochs=5, batch=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f818b2e4-fe30-4e6b-b669-c993b4e2fc1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.47  Python-3.11.7 torch-2.2.2+cpu CPU (11th Gen Intel Core(TM) i9-11900K 3.50GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=RefinedArtStyles, epochs=3, time=None, patience=100, batch=20, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train6, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\classify\\train6\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... found 4800 images in 6 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... found 1200 images in 6 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=1000 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    337926  ultralytics.nn.modules.head.Classify         [256, 6]                      \n",
      "YOLOv8n-cls summary: 99 layers, 1445974 parameters, 1445974 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\classify\\train6', view at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... 4800 images, 0 corrupt: 100%|██████████| 4800/4800 [\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... 1200 images, 0 corrupt: 100%|██████████| 1200/1200 [00:0\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.00046875), 27 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\classify\\train6\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/3         0G      1.371         20        224: 100%|██████████| 240/240 [03:07<00:00,  1.28it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 30/30 [00:33<00:00,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.673      0.996\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        2/3         0G     0.8742         20        224: 100%|██████████| 240/240 [03:07<00:00,  1.28it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 30/30 [00:33<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.752      0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        3/3         0G     0.7254         20        224: 100%|██████████| 240/240 [02:26<00:00,  1.63it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 30/30 [00:24<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.8          1\n",
      "\n",
      "3 epochs completed in 0.171 hours.\n",
      "Optimizer stripped from runs\\classify\\train6\\weights\\last.pt, 3.0MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from runs\\classify\\train6\\weights\\best.pt, 3.0MB\n",
      "\n",
      "Validating runs\\classify\\train6\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.1.47  Python-3.11.7 torch-2.2.2+cpu CPU (11th Gen Intel Core(TM) i9-11900K 3.50GHz)\n",
      "YOLOv8n-cls summary (fused): 73 layers, 1442566 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... found 4800 images in 6 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... found 1200 images in 6 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               classes   top1_acc   top5_acc: 100%|██████████| 30/30 [00:24<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        0.8          1\n",
      "Speed: 0.0ms preprocess, 3.3ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\classify\\train6\u001b[0m\n",
      "Results saved to \u001b[1mruns\\classify\\train6\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
       "\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x0000022C684FC1D0>\n",
       "curves: []\n",
       "curves_results: []\n",
       "fitness: 0.9000000059604645\n",
       "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
       "results_dict: {'metrics/accuracy_top1': 0.800000011920929, 'metrics/accuracy_top5': 1.0, 'fitness': 0.9000000059604645}\n",
       "save_dir: WindowsPath('runs/classify/train6')\n",
       "speed: {'preprocess': 0.0016609827677408855, 'inference': 3.3077943325042725, 'loss': 0.0, 'postprocess': 0.0}\n",
       "task: 'classify'\n",
       "top1: 0.800000011920929\n",
       "top5: 1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = YOLO('yolov8n-cls.pt')\n",
    "model3.train(data='RefinedArtStyles', epochs=3, batch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d17d91a0-a4ab-45c0-ae1f-b1cdf84b4851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.47  Python-3.11.7 torch-2.2.2+cpu CPU (11th Gen Intel Core(TM) i9-11900K 3.50GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=RefinedArtStyles, epochs=5, time=None, patience=100, batch=20, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train7, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\classify\\train7\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... found 4800 images in 6 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... found 1200 images in 6 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=1000 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    337926  ultralytics.nn.modules.head.Classify         [256, 6]                      \n",
      "YOLOv8n-cls summary: 99 layers, 1445974 parameters, 1445974 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\classify\\train7', view at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... 4800 images, 0 corrupt: 100%|██████████| 4800/4800 [\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... 1200 images, 0 corrupt: 100%|██████████| 1200/1200 [00:0\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.00046875), 27 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\classify\\train7\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.371         20        224: 100%|██████████| 240/240 [02:22<00:00,  1.69it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 30/30 [00:24<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.673      0.996\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        2/5         0G     0.8712         20        224: 100%|██████████| 240/240 [02:19<00:00,  1.72it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 30/30 [00:24<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.749      0.998\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        3/5         0G     0.7242         20        224: 100%|██████████| 240/240 [02:18<00:00,  1.73it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 30/30 [00:24<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.793          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        4/5         0G     0.6115         20        224: 100%|██████████| 240/240 [02:20<00:00,  1.71it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 30/30 [00:24<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.824          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        5/5         0G     0.5375         20        224: 100%|██████████| 240/240 [02:20<00:00,  1.71it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 30/30 [00:23<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.839      0.999\n",
      "\n",
      "5 epochs completed in 0.229 hours.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from runs\\classify\\train7\\weights\\last.pt, 3.0MB\n",
      "Optimizer stripped from runs\\classify\\train7\\weights\\best.pt, 3.0MB\n",
      "\n",
      "Validating runs\\classify\\train7\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.1.47  Python-3.11.7 torch-2.2.2+cpu CPU (11th Gen Intel Core(TM) i9-11900K 3.50GHz)\n",
      "YOLOv8n-cls summary (fused): 73 layers, 1442566 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... found 4800 images in 6 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... found 1200 images in 6 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               classes   top1_acc   top5_acc: 100%|██████████| 30/30 [00:23<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.838      0.999\n",
      "Speed: 0.0ms preprocess, 3.2ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\classify\\train7\u001b[0m\n",
      "Results saved to \u001b[1mruns\\classify\\train7\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
       "\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x0000022C03830410>\n",
       "curves: []\n",
       "curves_results: []\n",
       "fitness: 0.918749988079071\n",
       "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
       "results_dict: {'metrics/accuracy_top1': 0.8383333086967468, 'metrics/accuracy_top5': 0.9991666674613953, 'fitness': 0.918749988079071}\n",
       "save_dir: WindowsPath('runs/classify/train7')\n",
       "speed: {'preprocess': 0.0008285045623779297, 'inference': 3.1949156522750854, 'loss': 0.0, 'postprocess': 0.0}\n",
       "task: 'classify'\n",
       "top1: 0.8383333086967468\n",
       "top5: 0.9991666674613953"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4 = YOLO('yolov8n-cls.pt')\n",
    "model4.train(data='RefinedArtStyles', epochs=5, batch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8658095-81fb-4fb8-94b8-2da00a821178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.47  Python-3.11.7 torch-2.2.2+cpu CPU (11th Gen Intel Core(TM) i9-11900K 3.50GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=RefinedArtStyles, epochs=5, time=None, patience=100, batch=12, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train8, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\classify\\train8\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... found 4800 images in 6 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... found 1200 images in 6 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=1000 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    337926  ultralytics.nn.modules.head.Classify         [256, 6]                      \n",
      "YOLOv8n-cls summary: 99 layers, 1445974 parameters, 1445974 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\classify\\train8', view at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... 4800 images, 0 corrupt: 100%|██████████| 4800/4800 [\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... 1200 images, 0 corrupt: 100%|██████████| 1200/1200 [00:0\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.00046875), 27 bias(decay=0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\classify\\train8\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/5         0G      1.362         12        224: 100%|██████████| 400/400 [02:23<00:00,  2.79it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 50/50 [00:24<00:00,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.676      0.996\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        2/5         0G     0.8968         12        224: 100%|██████████| 400/400 [02:20<00:00,  2.84it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 50/50 [00:24<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.754      0.998\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        3/5         0G     0.7815         12        224: 100%|██████████| 400/400 [02:22<00:00,  2.81it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 50/50 [00:23<00:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.799      0.999\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        4/5         0G     0.6877         12        224: 100%|██████████| 400/400 [02:18<00:00,  2.89it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 50/50 [00:24<00:00,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.813          1\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "        5/5         0G      0.596         12        224: 100%|██████████| 400/400 [02:20<00:00,  2.85it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 50/50 [00:24<00:00,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.832          1\n",
      "\n",
      "5 epochs completed in 0.230 hours.\n",
      "Optimizer stripped from runs\\classify\\train8\\weights\\last.pt, 3.0MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from runs\\classify\\train8\\weights\\best.pt, 3.0MB\n",
      "\n",
      "Validating runs\\classify\\train8\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.1.47  Python-3.11.7 torch-2.2.2+cpu CPU (11th Gen Intel Core(TM) i9-11900K 3.50GHz)\n",
      "YOLOv8n-cls summary (fused): 73 layers, 1442566 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... found 4800 images in 6 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... found 1200 images in 6 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               classes   top1_acc   top5_acc: 100%|██████████| 50/50 [00:23<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.833          1\n",
      "Speed: 0.0ms preprocess, 3.2ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\classify\\train8\u001b[0m\n",
      "Results saved to \u001b[1mruns\\classify\\train8\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
       "\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x0000022C0337CD10>\n",
       "curves: []\n",
       "curves_results: []\n",
       "fitness: 0.9166666567325592\n",
       "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
       "results_dict: {'metrics/accuracy_top1': 0.8333333134651184, 'metrics/accuracy_top5': 1.0, 'fitness': 0.9166666567325592}\n",
       "save_dir: WindowsPath('runs/classify/train8')\n",
       "speed: {'preprocess': 0.0, 'inference': 3.215980728467305, 'loss': 0.0, 'postprocess': 0.0}\n",
       "task: 'classify'\n",
       "top1: 0.8333333134651184\n",
       "top5: 1.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5 = YOLO('yolov8n-cls.pt')\n",
    "model5.train(data='RefinedArtStyles', epochs=5, batch=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a51dca-f62d-4f16-ae56-a7da7b9bf12c",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f00ebdb4-c800-4709-ac49-7ccc7dbf262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.47  Python-3.11.7 torch-2.2.2+cpu CPU (11th Gen Intel Core(TM) i9-11900K 3.50GHz)\n",
      "YOLOv8n-cls summary (fused): 73 layers, 1442566 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... found 4800 images in 6 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... found 1200 images in 6 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... 1200 images, 0 corrupt: 100%|██████████| 1200/1200 [00:0\u001b[0m\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 75/75 [00:23<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.838      0.999\n",
      "Speed: 0.0ms preprocess, 3.1ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\classify\\val2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9991666674613953"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = \"Downloads/RefinedArtStyles/val\"\n",
    "\n",
    "#below model 1 is a placeholder: change the model name and the folder to use the better model among model1 and model 2\n",
    "model4 = YOLO(\"runs/classify/train7/weights/best.pt\")\n",
    "\n",
    "metrics4 = model4.val()\n",
    "metrics4.top1\n",
    "metrics4.top5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78a7fa9-7fe8-4286-9a86-5367dc4d8bd8",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8c3b10f5-daba-4238-b438-cdf20c4a66be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\kwkim\\Downloads\\..\\Downloads\\predict_img\\Baroque(5310).jpg: 224x224 Romanticism 0.57, Renaissance 0.30, Expressionism 0.05, Baroque 0.04, Realism 0.03, 10.0ms\n",
      "Speed: 12.0ms preprocess, 10.0ms inference, 0.0ms postprocess per image at shape (1, 3, 224, 224)\n",
      "Results saved to \u001b[1mruns\\classify\\predict\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Downloads/predict_img/Baroque(5311).jpg does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m model4 \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mruns/classify/train7/weights/best.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m m4_results \u001b[38;5;241m=\u001b[39m model4(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Downloads/predict_img/Baroque(5310).jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 5\u001b[0m m4_results \u001b[38;5;241m=\u001b[39m model4(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloads/predict_img/Baroque(5311).jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      6\u001b[0m m4_results \u001b[38;5;241m=\u001b[39m model4(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloads/predict_img/Baroque(5312).jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m m4_results \u001b[38;5;241m=\u001b[39m model4(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloads/predict_img/Cubism(1081).jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, save\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:176\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    155\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    156\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;124;03m    An alias for the predict method, enabling the model instance to be callable.\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m        (List[ultralytics.engine.results.Results]): A list of prediction results, encapsulated in the Results class.\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\ultralytics\\engine\\model.py:452\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 452\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:168\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m gen\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:220\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_model(model)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:  \u001b[38;5;66;03m# for thread-safe inference\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# Setup source every time predict is called\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_source(source \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msource)\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# Check if save_dir/ label file exists\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_txt:\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:192\u001b[0m, in \u001b[0;36mBasePredictor.setup_source\u001b[1;34m(self, source)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgsz \u001b[38;5;241m=\u001b[39m check_imgsz(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mimgsz, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mstride, min_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# check image size\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    191\u001b[0m )\n\u001b[1;32m--> 192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset \u001b[38;5;241m=\u001b[39m load_inference_source(\n\u001b[0;32m    193\u001b[0m     source\u001b[38;5;241m=\u001b[39msource,\n\u001b[0;32m    194\u001b[0m     batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[0;32m    195\u001b[0m     vid_stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvid_stride,\n\u001b[0;32m    196\u001b[0m     buffer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mstream_buffer,\n\u001b[0;32m    197\u001b[0m )\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39msource_type\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mstream\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mscreenshot\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1000\u001b[39m  \u001b[38;5;66;03m# many images\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo_flag\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;28;01mFalse\u001b[39;00m]))\n\u001b[0;32m    204\u001b[0m ):  \u001b[38;5;66;03m# videos\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\ultralytics\\data\\build.py:203\u001b[0m, in \u001b[0;36mload_inference_source\u001b[1;34m(source, batch, vid_stride, buffer)\u001b[0m\n\u001b[0;32m    201\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m LoadPilAndNumpy(source)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 203\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m LoadImagesAndVideos(source, batch\u001b[38;5;241m=\u001b[39mbatch, vid_stride\u001b[38;5;241m=\u001b[39mvid_stride)\n\u001b[0;32m    205\u001b[0m \u001b[38;5;66;03m# Attach source types to the dataset\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28msetattr\u001b[39m(dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_type\u001b[39m\u001b[38;5;124m\"\u001b[39m, source_type)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\Lib\\site-packages\\ultralytics\\data\\loaders.py:292\u001b[0m, in \u001b[0;36mLoadImagesAndVideos.__init__\u001b[1;34m(self, path, batch, vid_stride)\u001b[0m\n\u001b[0;32m    290\u001b[0m         files\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mstr\u001b[39m((parent \u001b[38;5;241m/\u001b[39m p)\u001b[38;5;241m.\u001b[39mabsolute()))  \u001b[38;5;66;03m# files (relative to *.txt file parent)\u001b[39;00m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# Define files as images or videos\u001b[39;00m\n\u001b[0;32m    295\u001b[0m images, videos \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Downloads/predict_img/Baroque(5311).jpg does not exist"
     ]
    }
   ],
   "source": [
    "#below model 1 is a placeholder: change the model name and the folder to use the better model among model1 and model 2\n",
    "model4 = YOLO(\"runs/classify/train7/weights/best.pt\")\n",
    "\n",
    "m4_results = model4('../Downloads/predict_img/Baroque(5310).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Baroque(5311).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Baroque(5312).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Cubism(1081).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Cubism(1109).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Cubism(1110).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Expressionism(4311).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Expressionism(4350).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Expressionism(4358).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Realism(5357).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Realism(5365).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Realism(5373).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Renaissance(6190).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Renaissance(6191).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Renaissance(6192).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Romanticism(6811).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Romanticism(6812).jpg', save=True)\n",
    "m4_results = model4('../Downloads/predict_img/Romanticism(6811).jpg', save=True)\n",
    "\n",
    "for r in m4_results:\n",
    "    print(r.probs, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d000b055-cf63-4dc1-ae5f-07ce62d48d3a",
   "metadata": {},
   "source": [
    "Experiment - Not used for the model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2eef045-1e4d-4557-a105-c65652555a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.47  Python-3.11.7 torch-2.2.2+cpu CPU (11th Gen Intel Core(TM) i9-11900K 3.50GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=classify, mode=train, model=yolov8n-cls.pt, data=RefinedArtStyles, epochs=1, time=None, patience=100, batch=16, imgsz=224, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\classify\\train3\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... found 4800 images in 6 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... found 1200 images in 6 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n",
      "Overriding model.yaml nc=1000 with nc=6\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    337926  ultralytics.nn.modules.head.Classify         [256, 6]                      \n",
      "YOLOv8n-cls summary: 99 layers, 1445974 parameters, 1445974 gradients, 3.4 GFLOPs\n",
      "Transferred 156/158 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\classify\\train3', view at http://localhost:6006/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... 4800 images, 0 corrupt: 100%|██████████| 4800/4800 [\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... 1200 images, 0 corrupt: 100%|██████████| 1200/1200 [00:0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val.cache\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 26 weight(decay=0.0), 27 weight(decay=0.0005), 27 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
      "Image sizes 224 train, 224 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\classify\\train3\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem       loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "        1/1         0G      1.364         16        224: 100%|██████████| 300/300 [03:23<00:00,  1.47it/s]\n",
      "               classes   top1_acc   top5_acc: 100%|██████████| 38/38 [00:35<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.668      0.997\n",
      "\n",
      "1 epochs completed in 0.067 hours.\n",
      "Optimizer stripped from runs\\classify\\train3\\weights\\last.pt, 3.0MB\n",
      "Optimizer stripped from runs\\classify\\train3\\weights\\best.pt, 3.0MB\n",
      "\n",
      "Validating runs\\classify\\train3\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.1.47  Python-3.11.7 torch-2.2.2+cpu CPU (11th Gen Intel Core(TM) i9-11900K 3.50GHz)\n",
      "YOLOv8n-cls summary (fused): 73 layers, 1442566 parameters, 0 gradients, 3.3 GFLOPs\n",
      "\u001b[34m\u001b[1mtrain:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\train... found 4800 images in 6 classes  \n",
      "\u001b[34m\u001b[1mval:\u001b[0m C:\\Users\\kwkim\\Downloads\\RefinedArtStyles\\val... found 1200 images in 6 classes  \n",
      "\u001b[34m\u001b[1mtest:\u001b[0m None...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "               classes   top1_acc   top5_acc: 100%|██████████| 38/38 [00:35<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all      0.668      0.997\n",
      "Speed: 0.0ms preprocess, 3.9ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\classify\\train3\u001b[0m\n",
      "Results saved to \u001b[1mruns\\classify\\train3\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ultralytics.utils.metrics.ClassifyMetrics object with attributes:\n",
       "\n",
       "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x0000022C025A7410>\n",
       "curves: []\n",
       "curves_results: []\n",
       "fitness: 0.8325000107288361\n",
       "keys: ['metrics/accuracy_top1', 'metrics/accuracy_top5']\n",
       "results_dict: {'metrics/accuracy_top1': 0.6683333516120911, 'metrics/accuracy_top5': 0.996666669845581, 'fitness': 0.8325000107288361}\n",
       "save_dir: WindowsPath('runs/classify/train3')\n",
       "speed: {'preprocess': 0.0, 'inference': 3.8737869262695312, 'loss': 0.0, 'postprocess': 0.0}\n",
       "task: 'classify'\n",
       "top1: 0.6683333516120911\n",
       "top5: 0.996666669845581"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = YOLO('yolov8n-cls.pt')\n",
    "model1.train(data='RefinedArtStyles', epochs=1, batch=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
